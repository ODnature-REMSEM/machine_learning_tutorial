{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geospatial data splitting\n",
    "\n",
    "When doing machine learning on geospatial data, special care must be taken to prevent data leakage due to spatiotemporal correlation (for example, when a certain pixel is in the training set, it could influence the prediction of a neighbouring pixel in the test set).\n",
    "\n",
    "In order to prevent this, it's a good idea to take a different approach than just splitting pixels randomly. There are a few ways to handle this, including:\n",
    "* Temporal splitting: Where you train on a certain timerange in the data, and evaluate on a different timerange\n",
    "* Spatial splitting: \n",
    "* In either of the cases above, you can choose to just pick a splitting point (e.g. a everything above a certain x coordiante, everything before a certain timestamp, ...) or you can do it stratified (every 5th month is in the test set, separating the data in strips that are in the train or test set, dividing your data in a grid, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strip-based splitting\n",
    "\n",
    "An example to do this is shown below, where we divide the satellite image in different strips, based on the latitude of the image. The goal is to have a train and a test set, that each consist of different strips going from north to south, to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load in the satellite data as in the previous notebook, but we also need to keep the coordinate values this time, as they are used for train / test splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('data/s3_20200420T101527.nc')\n",
    "df = ds.to_dataframe()\n",
    "\n",
    "features = ['Rrs400_a', 'Rrs412_a', 'Rrs443_a', 'Rrs490_a', 'Rrs510_a',\n",
    "            'Rrs560_a', 'Rrs620_a', 'Rrs665_a', 'Rrs674_a', 'Rrs682_a',\n",
    "            'Rrs709_a', 'Rrs754_a', 'Rrs768_a', 'Rrs779_a',\n",
    "            'Rrs865_a', 'Rrs884_a']\n",
    "\n",
    "target = 'CHL'\n",
    "\n",
    "df = df.reset_index()\n",
    "df = df[features + [target] + ['lat', 'lon']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we choose to divide the data up into 50 horizontal strips. A new column is added indicating which strip each data point belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_strips = 50\n",
    "strips = range(n_strips)\n",
    "df['lat_strip'] = pd.cut(df['lat'], n_strips, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 721744\n",
      "Number of test samples: 193005\n"
     ]
    }
   ],
   "source": [
    "train_strips, test_strips = train_test_split(strips, test_size=0.2, random_state=42)\n",
    "\n",
    "df_train = df[df['lat_strip'].isin(train_strips)]\n",
    "df_test = df[df['lat_strip'].isin(test_strips)]\n",
    "\n",
    "print(f'Number of train samples: {len(df_train)}')\n",
    "print(f'Number of test samples: {len(df_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 725273, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 4.743175\n",
      "Mean squared error: 4.8316257658220225\n"
     ]
    }
   ],
   "source": [
    "X_train = df_train[features]\n",
    "y_train = df_train[target]\n",
    "X_test = df_test[features]\n",
    "y_test = df_test[target]\n",
    "\n",
    "model = LGBMRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean squared error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the error is significantly higher than in the previous notebook. This does not necessarily mean that the model performed worse. A more likely explanation is that this method avoided data leakage, and that this error is more representative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid-based splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('data/s3_20200420T101527.nc')\n",
    "df = ds.to_dataframe()\n",
    "\n",
    "features = ['Rrs400_a', 'Rrs412_a', 'Rrs443_a', 'Rrs490_a', 'Rrs510_a',\n",
    "            'Rrs560_a', 'Rrs620_a', 'Rrs665_a', 'Rrs674_a', 'Rrs682_a',\n",
    "            'Rrs709_a', 'Rrs754_a', 'Rrs768_a', 'Rrs779_a',\n",
    "            'Rrs865_a', 'Rrs884_a']\n",
    "\n",
    "target = 'CHL'\n",
    "\n",
    "df = df.reset_index()\n",
    "df = df[features + [target] + ['lat', 'lon']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a number of strips in the latitude and longitude direction which will make up the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of grid cells that contain data: 257\n"
     ]
    }
   ],
   "source": [
    "n_strips_lat = 20\n",
    "n_strips_lon = 20\n",
    "strips = range(n_strips)\n",
    "df['grid_lat'] = pd.cut(df['lat'], n_strips_lat, labels=False)\n",
    "df['grid_lon'] = pd.cut(df['lon'], n_strips_lon, labels=False)\n",
    "\n",
    "# Get the grid cells that actually contain data\n",
    "unique_grid_cells = df[['grid_lon', 'grid_lat']].drop_duplicates()\n",
    "print(f\"Number of grid cells that contain data: {len(unique_grid_cells)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate into train and test grid cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cells, test_cells = train_test_split(unique_grid_cells.index, test_size=0.2, random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
