{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "Machine learning models always have a set of parameters that can be set. Some examples are the maximum depth of a tree in a tree-based model, or the amount of trees to use. These are called hyperparameters, and the optimal combination depends on each individual problem.\n",
    "\n",
    "Tuning these parameters can be done in different ways. Usually, a dictionary is set up containing the different possibilities to try out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/chl_regression_tutorial.csv')\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "features = ['rho_443_a', 'rho_492_a', 'rho_560_a', 'rho_665_a', 'rho_704_a', 'rho_740_a', 'rho_783_a', 'rho_865_a']\n",
    "target = 'CHL'\n",
    "\n",
    "X_train = df_train[features]\n",
    "y_train = df_train[target]\n",
    "\n",
    "X_test = df_test[features]\n",
    "y_test = df_test[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search\n",
    "One way to find the optimal parameters is the \"brute-force\" method, by trying out each individual combination. This is called grid-search, where you define a \"hyperparameter grid\", and evaluate the performance of each combination using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Best hyperparameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = lgb.LGBMRegressor()\n",
    "\n",
    "hyperparameter_search_space = {\n",
    "    'n_estimators': [10, 100, 1000],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "}\n",
    "\n",
    "search = GridSearchCV(model, hyperparameter_search_space, n_jobs=-1, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print('Best hyperparameters:', search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then train the model using the found parameters and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 3.017702545136178\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMRegressor(**search.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print('Test MSE:', mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search\n",
    "\n",
    "When your search space is large, an exhaustive search as is done in grid search can be unfeasible. In many cases, random search also does a good job, where random combinations from your set are tried out, and you retain the combination with the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best hyperparameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 416}\n",
      "Test MSE: 2.971126907243025\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "model = lgb.LGBMRegressor()\n",
    "\n",
    "hyperparameter_search_space = {\n",
    "    'n_estimators': sp_randint(10, 1000),\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'max_depth': sp_randint(3, 10),\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(model, hyperparameter_search_space, n_iter=10, n_jobs=-1, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
    "search.fit(X_train, y_train)\n",
    "print('Best hyperparameters:', search.best_params_)\n",
    "\n",
    "model = lgb.LGBMRegressor(**search.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print('Test MSE:', mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also python packages such as Optuna that implemeny more complex hyperparameter tuning strategies. Instead of doing a random search, Optuna's algorithm tries to identify the most promising area's of the search space based on prior evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 16:03:31,147] A new study created in memory with name: no-name-d6ef3045-1624-4a6d-984c-98c7d6daa4a2\n",
      "[I 2024-10-30 16:03:31,461] Trial 0 finished with value: 3.9582895354072942 and parameters: {'n_estimators': 381, 'learning_rate': 0.711447600934342, 'max_depth': 8}. Best is trial 0 with value: 3.9582895354072942.\n",
      "[I 2024-10-30 16:03:31,733] Trial 1 finished with value: 6.276020288307635 and parameters: {'n_estimators': 603, 'learning_rate': 0.0029380279387035343, 'max_depth': 4}. Best is trial 0 with value: 3.9582895354072942.\n",
      "[I 2024-10-30 16:03:31,790] Trial 2 finished with value: 3.5012417575314347 and parameters: {'n_estimators': 67, 'learning_rate': 0.39676050770529875, 'max_depth': 7}. Best is trial 2 with value: 3.5012417575314347.\n",
      "[I 2024-10-30 16:03:32,406] Trial 3 finished with value: 9.618441508331184 and parameters: {'n_estimators': 711, 'learning_rate': 0.00115279871282324, 'max_depth': 10}. Best is trial 2 with value: 3.5012417575314347.\n",
      "[I 2024-10-30 16:03:32,750] Trial 4 finished with value: 4.295370052122822 and parameters: {'n_estimators': 834, 'learning_rate': 0.004335281794951566, 'max_depth': 4}. Best is trial 2 with value: 3.5012417575314347.\n",
      "[I 2024-10-30 16:03:32,918] Trial 5 finished with value: 5.252697739434259 and parameters: {'n_estimators': 191, 'learning_rate': 0.008179499475211672, 'max_depth': 7}. Best is trial 2 with value: 3.5012417575314347.\n",
      "[I 2024-10-30 16:03:33,277] Trial 6 finished with value: 3.5027063452499876 and parameters: {'n_estimators': 438, 'learning_rate': 0.007476312062252299, 'max_depth': 7}. Best is trial 2 with value: 3.5012417575314347.\n",
      "[I 2024-10-30 16:03:33,388] Trial 7 finished with value: 7.645451528825035 and parameters: {'n_estimators': 148, 'learning_rate': 0.007523742884534853, 'max_depth': 5}. Best is trial 2 with value: 3.5012417575314347.\n",
      "[I 2024-10-30 16:03:33,543] Trial 8 finished with value: 3.1893496049776613 and parameters: {'n_estimators': 461, 'learning_rate': 0.22673986523780384, 'max_depth': 4}. Best is trial 8 with value: 3.1893496049776613.\n",
      "[I 2024-10-30 16:03:33,700] Trial 9 finished with value: 3.059911920769038 and parameters: {'n_estimators': 519, 'learning_rate': 0.05987474910461398, 'max_depth': 3}. Best is trial 9 with value: 3.059911920769038.\n",
      "[I 2024-10-30 16:03:33,960] Trial 10 finished with value: 2.940196498692981 and parameters: {'n_estimators': 946, 'learning_rate': 0.08909353022689835, 'max_depth': 3}. Best is trial 10 with value: 2.940196498692981.\n",
      "[I 2024-10-30 16:03:34,204] Trial 11 finished with value: 2.945093154372252 and parameters: {'n_estimators': 933, 'learning_rate': 0.07469521320105203, 'max_depth': 3}. Best is trial 10 with value: 2.940196498692981.\n",
      "[I 2024-10-30 16:03:34,472] Trial 12 finished with value: 2.9182126012789236 and parameters: {'n_estimators': 984, 'learning_rate': 0.05536361692963019, 'max_depth': 3}. Best is trial 12 with value: 2.9182126012789236.\n",
      "[I 2024-10-30 16:03:34,925] Trial 13 finished with value: 3.114987010347442 and parameters: {'n_estimators': 998, 'learning_rate': 0.11855595167198668, 'max_depth': 5}. Best is trial 12 with value: 2.9182126012789236.\n",
      "[I 2024-10-30 16:03:35,322] Trial 14 finished with value: 3.022625658979223 and parameters: {'n_estimators': 801, 'learning_rate': 0.024818212736695242, 'max_depth': 5}. Best is trial 12 with value: 2.9182126012789236.\n",
      "[I 2024-10-30 16:03:35,587] Trial 15 finished with value: 3.52133923146513 and parameters: {'n_estimators': 873, 'learning_rate': 0.019113354590616624, 'max_depth': 3}. Best is trial 12 with value: 2.9182126012789236.\n",
      "[I 2024-10-30 16:03:36,049] Trial 16 finished with value: 3.129349127937839 and parameters: {'n_estimators': 703, 'learning_rate': 0.19950000057046405, 'max_depth': 9}. Best is trial 12 with value: 2.9182126012789236.\n",
      "[I 2024-10-30 16:03:36,571] Trial 17 finished with value: 2.9323506623891062 and parameters: {'n_estimators': 999, 'learning_rate': 0.03636507744176543, 'max_depth': 6}. Best is trial 12 with value: 2.9182126012789236.\n",
      "[I 2024-10-30 16:03:36,971] Trial 18 finished with value: 3.000020748678149 and parameters: {'n_estimators': 704, 'learning_rate': 0.033415876088381084, 'max_depth': 6}. Best is trial 12 with value: 2.9182126012789236.\n",
      "[I 2024-10-30 16:03:37,548] Trial 19 finished with value: 2.9720352536409496 and parameters: {'n_estimators': 997, 'learning_rate': 0.042285565379044446, 'max_depth': 6}. Best is trial 12 with value: 2.9182126012789236.\n",
      "[I 2024-10-30 16:03:37,812] Trial 20 finished with value: 3.2539930028187847 and parameters: {'n_estimators': 316, 'learning_rate': 0.015582729494622046, 'max_depth': 8}. Best is trial 12 with value: 2.9182126012789236.\n",
      "[I 2024-10-30 16:03:38,197] Trial 21 finished with value: 2.873388655751031 and parameters: {'n_estimators': 906, 'learning_rate': 0.11606159532098595, 'max_depth': 4}. Best is trial 21 with value: 2.873388655751031.\n",
      "[I 2024-10-30 16:03:38,468] Trial 22 finished with value: 2.891176700088205 and parameters: {'n_estimators': 793, 'learning_rate': 0.14756583595023567, 'max_depth': 4}. Best is trial 21 with value: 2.873388655751031.\n",
      "[I 2024-10-30 16:03:38,758] Trial 23 finished with value: 2.8241203257979968 and parameters: {'n_estimators': 789, 'learning_rate': 0.17469892838723067, 'max_depth': 4}. Best is trial 23 with value: 2.8241203257979968.\n",
      "[I 2024-10-30 16:03:39,164] Trial 24 finished with value: 2.9821783410082063 and parameters: {'n_estimators': 775, 'learning_rate': 0.23436732632306218, 'max_depth': 4}. Best is trial 23 with value: 2.8241203257979968.\n",
      "[I 2024-10-30 16:03:39,444] Trial 25 finished with value: 5.111898012801509 and parameters: {'n_estimators': 553, 'learning_rate': 0.9524539638196022, 'max_depth': 5}. Best is trial 23 with value: 2.8241203257979968.\n",
      "[I 2024-10-30 16:03:39,679] Trial 26 finished with value: 2.8259709687850516 and parameters: {'n_estimators': 629, 'learning_rate': 0.12325928549462167, 'max_depth': 4}. Best is trial 23 with value: 2.8241203257979968.\n",
      "[I 2024-10-30 16:03:39,961] Trial 27 finished with value: 3.706223750168352 and parameters: {'n_estimators': 614, 'learning_rate': 0.43018196629469085, 'max_depth': 5}. Best is trial 23 with value: 2.8241203257979968.\n",
      "[I 2024-10-30 16:03:40,280] Trial 28 finished with value: 3.5326421905352348 and parameters: {'n_estimators': 884, 'learning_rate': 0.4514984692571629, 'max_depth': 4}. Best is trial 23 with value: 2.8241203257979968.\n",
      "[I 2024-10-30 16:03:40,650] Trial 29 finished with value: 3.0871364550772635 and parameters: {'n_estimators': 630, 'learning_rate': 0.10980663379999392, 'max_depth': 6}. Best is trial 23 with value: 2.8241203257979968.\n",
      "[I 2024-10-30 16:03:40,807] Trial 30 finished with value: 3.1440890990297623 and parameters: {'n_estimators': 317, 'learning_rate': 0.3067059094420892, 'max_depth': 5}. Best is trial 23 with value: 2.8241203257979968.\n",
      "[I 2024-10-30 16:03:41,092] Trial 31 finished with value: 2.874171562527239 and parameters: {'n_estimators': 812, 'learning_rate': 0.15062191159752017, 'max_depth': 4}. Best is trial 23 with value: 2.8241203257979968.\n",
      "[I 2024-10-30 16:03:41,410] Trial 32 finished with value: 2.918153897596534 and parameters: {'n_estimators': 730, 'learning_rate': 0.15675521557954686, 'max_depth': 4}. Best is trial 23 with value: 2.8241203257979968.\n",
      "[I 2024-10-30 16:03:41,765] Trial 33 finished with value: 3.513406648372579 and parameters: {'n_estimators': 650, 'learning_rate': 0.5556294386496017, 'max_depth': 3}. Best is trial 23 with value: 2.8241203257979968.\n",
      "[I 2024-10-30 16:03:42,241] Trial 34 finished with value: 3.1837234983230824 and parameters: {'n_estimators': 875, 'learning_rate': 0.28938864930077346, 'max_depth': 4}. Best is trial 23 with value: 2.8241203257979968.\n",
      "[I 2024-10-30 16:03:42,636] Trial 35 finished with value: 2.878227355923811 and parameters: {'n_estimators': 754, 'learning_rate': 0.08413358059343805, 'max_depth': 4}. Best is trial 23 with value: 2.8241203257979968.\n",
      "[I 2024-10-30 16:03:43,252] Trial 36 finished with value: 4.011185657751561 and parameters: {'n_estimators': 833, 'learning_rate': 0.6503438640924862, 'max_depth': 5}. Best is trial 23 with value: 2.8241203257979968.\n",
      "[I 2024-10-30 16:03:43,489] Trial 37 finished with value: 2.9869411974038385 and parameters: {'n_estimators': 677, 'learning_rate': 0.15817805268265592, 'max_depth': 3}. Best is trial 23 with value: 2.8241203257979968.\n",
      "[I 2024-10-30 16:03:44,102] Trial 38 finished with value: 8.389003810272023 and parameters: {'n_estimators': 570, 'learning_rate': 0.0016690290731762446, 'max_depth': 8}. Best is trial 23 with value: 2.8241203257979968.\n",
      "[I 2024-10-30 16:03:44,698] Trial 39 finished with value: 3.1272815951161026 and parameters: {'n_estimators': 902, 'learning_rate': 0.30656804082927824, 'max_depth': 4}. Best is trial 23 with value: 2.8241203257979968.\n",
      "[I 2024-10-30 16:03:45,544] Trial 40 finished with value: 2.997393165379544 and parameters: {'n_estimators': 831, 'learning_rate': 0.05486318464713464, 'max_depth': 10}. Best is trial 23 with value: 2.8241203257979968.\n",
      "[I 2024-10-30 16:03:45,874] Trial 41 finished with value: 2.9547598601148803 and parameters: {'n_estimators': 747, 'learning_rate': 0.09516785448812316, 'max_depth': 4}. Best is trial 23 with value: 2.8241203257979968.\n",
      "[I 2024-10-30 16:03:46,202] Trial 42 finished with value: 2.8650817147570615 and parameters: {'n_estimators': 780, 'learning_rate': 0.07548424649133434, 'max_depth': 4}. Best is trial 23 with value: 2.8241203257979968.\n",
      "[I 2024-10-30 16:03:46,711] Trial 43 finished with value: 2.9811950971419887 and parameters: {'n_estimators': 932, 'learning_rate': 0.12826486657055833, 'max_depth': 5}. Best is trial 23 with value: 2.8241203257979968.\n",
      "[I 2024-10-30 16:03:46,991] Trial 44 finished with value: 2.9577509057839126 and parameters: {'n_estimators': 806, 'learning_rate': 0.06293829525740104, 'max_depth': 3}. Best is trial 23 with value: 2.8241203257979968.\n",
      "[I 2024-10-30 16:03:47,192] Trial 45 finished with value: 2.7996390799736965 and parameters: {'n_estimators': 487, 'learning_rate': 0.19025233657562704, 'max_depth': 4}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:47,360] Trial 46 finished with value: 3.052429893416993 and parameters: {'n_estimators': 467, 'learning_rate': 0.18826734434934136, 'max_depth': 3}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:47,785] Trial 47 finished with value: 2.997245563123553 and parameters: {'n_estimators': 503, 'learning_rate': 0.04682609593144581, 'max_depth': 5}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:48,110] Trial 48 finished with value: 3.037330490980061 and parameters: {'n_estimators': 395, 'learning_rate': 0.07309222981501617, 'max_depth': 7}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:48,344] Trial 49 finished with value: 3.57004358561378 and parameters: {'n_estimators': 576, 'learning_rate': 0.026680804054199425, 'max_depth': 3}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:48,529] Trial 50 finished with value: 3.0057672955800427 and parameters: {'n_estimators': 401, 'learning_rate': 0.23940760647056528, 'max_depth': 4}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:48,827] Trial 51 finished with value: 2.8535156963096564 and parameters: {'n_estimators': 662, 'learning_rate': 0.11843779521593702, 'max_depth': 4}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:49,095] Trial 52 finished with value: 2.849789872537947 and parameters: {'n_estimators': 678, 'learning_rate': 0.10521080199238517, 'max_depth': 4}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:49,433] Trial 53 finished with value: 2.936746022772282 and parameters: {'n_estimators': 669, 'learning_rate': 0.08904103438346467, 'max_depth': 5}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:49,660] Trial 54 finished with value: 2.8359080722494143 and parameters: {'n_estimators': 519, 'learning_rate': 0.18947164137557432, 'max_depth': 4}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:49,842] Trial 55 finished with value: 3.0071935723943732 and parameters: {'n_estimators': 508, 'learning_rate': 0.32021944177383455, 'max_depth': 3}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:50,139] Trial 56 finished with value: 2.930900351842192 and parameters: {'n_estimators': 541, 'learning_rate': 0.18819074572421232, 'max_depth': 5}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:50,527] Trial 57 finished with value: 3.344896290983457 and parameters: {'n_estimators': 464, 'learning_rate': 0.3876381179166773, 'max_depth': 9}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:50,786] Trial 58 finished with value: 3.289783337223345 and parameters: {'n_estimators': 597, 'learning_rate': 0.2451365907396481, 'max_depth': 4}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:51,012] Trial 59 finished with value: 2.9932149812975823 and parameters: {'n_estimators': 690, 'learning_rate': 0.11236717449625655, 'max_depth': 3}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:51,173] Trial 60 finished with value: 3.1119720274443914 and parameters: {'n_estimators': 337, 'learning_rate': 0.18192096129074484, 'max_depth': 4}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:51,436] Trial 61 finished with value: 2.9556510260328404 and parameters: {'n_estimators': 635, 'learning_rate': 0.06814609607414134, 'max_depth': 4}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:51,800] Trial 62 finished with value: 2.9163652596744774 and parameters: {'n_estimators': 715, 'learning_rate': 0.12295351175344062, 'max_depth': 5}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:52,168] Trial 63 finished with value: 2.9689300611371348 and parameters: {'n_estimators': 584, 'learning_rate': 0.04045483958164872, 'max_depth': 4}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:52,254] Trial 64 finished with value: 3.5379610668345842 and parameters: {'n_estimators': 70, 'learning_rate': 0.09799156005529064, 'max_depth': 4}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:52,526] Trial 65 finished with value: 2.991864181298172 and parameters: {'n_estimators': 426, 'learning_rate': 0.13774426705638865, 'max_depth': 6}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:52,776] Trial 66 finished with value: 2.9781415381925775 and parameters: {'n_estimators': 762, 'learning_rate': 0.05030562525239103, 'max_depth': 3}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:53,002] Trial 67 finished with value: 2.8925639816098583 and parameters: {'n_estimators': 532, 'learning_rate': 0.21672314528610379, 'max_depth': 4}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:53,482] Trial 68 finished with value: 3.5495701364245713 and parameters: {'n_estimators': 650, 'learning_rate': 0.005799783974095301, 'max_depth': 5}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:53,692] Trial 69 finished with value: 2.9063166089637726 and parameters: {'n_estimators': 497, 'learning_rate': 0.07462810338883698, 'max_depth': 4}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:53,886] Trial 70 finished with value: 3.2102621356800563 and parameters: {'n_estimators': 606, 'learning_rate': 0.3764834145672658, 'max_depth': 3}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:54,251] Trial 71 finished with value: 2.9299584962979393 and parameters: {'n_estimators': 957, 'learning_rate': 0.10673904352744303, 'max_depth': 4}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:54,573] Trial 72 finished with value: 3.245564751943039 and parameters: {'n_estimators': 784, 'learning_rate': 0.01305722927132683, 'max_depth': 4}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:55,006] Trial 73 finished with value: 3.042125558674943 and parameters: {'n_estimators': 848, 'learning_rate': 0.16191568272322374, 'max_depth': 5}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:55,271] Trial 74 finished with value: 3.156256734680847 and parameters: {'n_estimators': 733, 'learning_rate': 0.26203297089359817, 'max_depth': 4}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:55,608] Trial 75 finished with value: 2.9029194697946057 and parameters: {'n_estimators': 910, 'learning_rate': 0.12936016851857135, 'max_depth': 4}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:55,825] Trial 76 finished with value: 3.006566749447789 and parameters: {'n_estimators': 654, 'learning_rate': 0.08774872856852241, 'max_depth': 3}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:56,110] Trial 77 finished with value: 2.8306761737126935 and parameters: {'n_estimators': 691, 'learning_rate': 0.17210148254223614, 'max_depth': 4}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:56,439] Trial 78 finished with value: 3.052448064368734 and parameters: {'n_estimators': 689, 'learning_rate': 0.19445808778825363, 'max_depth': 5}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:56,650] Trial 79 finished with value: 3.3936115632285264 and parameters: {'n_estimators': 556, 'learning_rate': 0.4664672166202032, 'max_depth': 4}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:56,879] Trial 80 finished with value: 2.945458909299901 and parameters: {'n_estimators': 719, 'learning_rate': 0.15921551828247874, 'max_depth': 3}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:57,158] Trial 81 finished with value: 2.8423487927734428 and parameters: {'n_estimators': 756, 'learning_rate': 0.10935124129577106, 'max_depth': 4}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:57,410] Trial 82 finished with value: 2.8781276098591535 and parameters: {'n_estimators': 625, 'learning_rate': 0.0833349647591999, 'max_depth': 4}. Best is trial 45 with value: 2.7996390799736965.\n",
      "[I 2024-10-30 16:03:57,712] Trial 83 finished with value: 2.7783331409279315 and parameters: {'n_estimators': 773, 'learning_rate': 0.14482642335895063, 'max_depth': 4}. Best is trial 83 with value: 2.7783331409279315.\n",
      "[I 2024-10-30 16:03:57,988] Trial 84 finished with value: 3.0183717733382074 and parameters: {'n_estimators': 740, 'learning_rate': 0.21500943696988004, 'max_depth': 4}. Best is trial 83 with value: 2.7783331409279315.\n",
      "[I 2024-10-30 16:03:58,408] Trial 85 finished with value: 3.417647266335068 and parameters: {'n_estimators': 846, 'learning_rate': 0.3378023646353403, 'max_depth': 5}. Best is trial 83 with value: 2.7783331409279315.\n",
      "[I 2024-10-30 16:03:58,698] Trial 86 finished with value: 2.9504106637626983 and parameters: {'n_estimators': 700, 'learning_rate': 0.13686800183953263, 'max_depth': 4}. Best is trial 83 with value: 2.7783331409279315.\n",
      "[I 2024-10-30 16:03:58,933] Trial 87 finished with value: 3.167696920985004 and parameters: {'n_estimators': 669, 'learning_rate': 0.2807892576417708, 'max_depth': 3}. Best is trial 83 with value: 2.7783331409279315.\n",
      "[I 2024-10-30 16:03:59,498] Trial 88 finished with value: 3.1642714574531827 and parameters: {'n_estimators': 767, 'learning_rate': 0.1725542370748742, 'max_depth': 7}. Best is trial 83 with value: 2.7783331409279315.\n",
      "[I 2024-10-30 16:03:59,826] Trial 89 finished with value: 2.835247584276769 and parameters: {'n_estimators': 818, 'learning_rate': 0.0592061274513634, 'max_depth': 4}. Best is trial 83 with value: 2.7783331409279315.\n",
      "[I 2024-10-30 16:04:00,317] Trial 90 finished with value: 2.9501394728914385 and parameters: {'n_estimators': 802, 'learning_rate': 0.055278477419915005, 'max_depth': 5}. Best is trial 83 with value: 2.7783331409279315.\n",
      "[I 2024-10-30 16:04:00,651] Trial 91 finished with value: 2.9867972502771485 and parameters: {'n_estimators': 817, 'learning_rate': 0.11081500585945227, 'max_depth': 4}. Best is trial 83 with value: 2.7783331409279315.\n",
      "[I 2024-10-30 16:04:00,943] Trial 92 finished with value: 2.931369021146905 and parameters: {'n_estimators': 758, 'learning_rate': 0.09711123990202215, 'max_depth': 4}. Best is trial 83 with value: 2.7783331409279315.\n",
      "[I 2024-10-30 16:04:01,275] Trial 93 finished with value: 2.954674257177079 and parameters: {'n_estimators': 870, 'learning_rate': 0.14878415582220247, 'max_depth': 4}. Best is trial 83 with value: 2.7783331409279315.\n",
      "[I 2024-10-30 16:04:01,565] Trial 94 finished with value: 2.8522676436193777 and parameters: {'n_estimators': 724, 'learning_rate': 0.12438875991171552, 'max_depth': 4}. Best is trial 83 with value: 2.7783331409279315.\n",
      "[I 2024-10-30 16:04:01,831] Trial 95 finished with value: 2.8801758837330236 and parameters: {'n_estimators': 713, 'learning_rate': 0.06409318928286167, 'max_depth': 4}. Best is trial 83 with value: 2.7783331409279315.\n",
      "[I 2024-10-30 16:04:01,951] Trial 96 finished with value: 3.039982626683238 and parameters: {'n_estimators': 229, 'learning_rate': 0.2035394369481744, 'max_depth': 4}. Best is trial 83 with value: 2.7783331409279315.\n",
      "[I 2024-10-30 16:04:02,172] Trial 97 finished with value: 3.196639010327733 and parameters: {'n_estimators': 779, 'learning_rate': 0.25627577985693045, 'max_depth': 3}. Best is trial 83 with value: 2.7783331409279315.\n",
      "[I 2024-10-30 16:04:02,584] Trial 98 finished with value: 2.9868126003429065 and parameters: {'n_estimators': 819, 'learning_rate': 0.13254473759134605, 'max_depth': 5}. Best is trial 83 with value: 2.7783331409279315.\n",
      "[I 2024-10-30 16:04:02,795] Trial 99 finished with value: 2.857310300555626 and parameters: {'n_estimators': 490, 'learning_rate': 0.10201979888090675, 'max_depth': 4}. Best is trial 83 with value: 2.7783331409279315.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'n_estimators': 773, 'learning_rate': 0.14482642335895063, 'max_depth': 4}\n",
      "Test MSE: 2.7783331409279315\n"
     ]
    }
   ],
   "source": [
    "# using Optuna\n",
    "import optuna\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 1000)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-3, 1, log=True)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "\n",
    "    model = lgb.LGBMRegressor(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    return mean_squared_error(y_test, y_pred)\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print('Best hyperparameters:', study.best_params)\n",
    "\n",
    "model = lgb.LGBMRegressor(**study.best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print('Test MSE:', mean_squared_error(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
